Martin Escardo, Paulo Oliva, 2nd July 2021

We study finite, history dependent games of perfect information using
selection functions and dependent-type trees.

This is based on our previous work.

  [1] M.H. Escard√≥ and Paulo Oliva. Sequential Games and Optimal
      Strategies. Proceedings of the Royal Society A, 467:1519-1545,
      2011. https://doi.org/10.1098/rspa.2010.0471

We generalize [1] by considering the case that the type X‚Çñ of moves x‚Çñ
at round k depends on the moves played at the previous rounds. So in a
sequence of moves x‚ÇÄ,x‚ÇÅ,x‚ÇÇ,‚Ä¶, we have that

  * the type X‚ÇÄ of initial moves doesn't depend on anything,
  * the type X‚ÇÅ depends on the first move x‚ÇÄ : X‚ÇÄ,
  * the type X‚ÇÇ depends on depends on the first move x‚ÇÄ : X‚ÇÄ and the
    second move x‚ÇÅ : X‚ÇÅ.

In order to achieve this generalization, we work with game trees
whose nodes are labelled by types that collect the allowed moves at a
given round. Then a play x‚ÇÄ,x‚ÇÅ,x‚ÇÇ,‚Ä¶, is a path in such a tree.

This formulation of the notion of game naturally accounts for finite
games of *unbounded* length, which in [1] was achieved by continuous,
infinite games instead.

\begin{code}

{-# OPTIONS --without-K --exact-split --safe #-}

open import SpartanMLTT hiding (J)
open import UF-Base
open import UF-FunExt

module FiniteHistoryDependentGames (fe : Fun-Ext) where

\end{code}

We represent the moves of a history-dependent sequential game by a
dependent-type tree (DTT).  This is either an empty tree [] or else
has a type X of initial moves at the root, and, inductively, a family
Xf of subtrees indexed by elements of X, which is written X ‚à∑ Xf. We
refer to the family Xf as a forest. We let Xt range over such trees.

 * Xt ranges over dependent-type trees.
 * Xf ranges over dependent-type forests.

\begin{code}

data DTT : Type‚ÇÅ where
  []  : DTT
  _‚à∑_ : (X : Type) (Xf : X ‚Üí DTT) ‚Üí DTT

\end{code}

The type of full paths in a tree Xt, from the root to a leaf, is
inductively defined as follows:

\begin{code}

Path : DTT ‚Üí Type
Path []       = ùüô
Path (X ‚à∑ Xf) = Œ£ x Íûâ X , Path (Xf x)

\end{code}

As discussed above, a play in a game is defined to be such a path.

The variable xs ranges over paths, that is, elements of the type
Path Xt for a dependent-type-tree Xt.

\begin{code}

pattern _::_ x xs = (x , xs)

path-head : {X : Type} {Xf : X ‚Üí DTT} ‚Üí Path (X ‚à∑ Xf) ‚Üí X
path-head (x :: xs) = x

path-tail : {X : Type} {Xf : X ‚Üí DTT} ((x :: xs) : Path (X ‚à∑ Xf)) ‚Üí Path (Xf x)
path-tail (x :: xs) = xs

\end{code}

The idea is that we choose a move x, and then, inductively, a path in
the subtree Xf x.

Quantifiers and selections, as in Sections 1 and 2 of reference [1]:

\begin{code}

K : Type ‚Üí Type ‚Üí Type
K R X = (X ‚Üí R) ‚Üí R

J : Type ‚Üí Type ‚Üí Type
J R X = (X ‚Üí R) ‚Üí X

\end{code}

In the same way as the type of moves at a given stage of the game
depends on the previousely played moves, so do the quantifies and
selection functions.

ùìö assigns a quantifier to each node in a given tree, and similarly ùìô
assigns selection functions to the nodes.

\begin{code}

ùìö :  Type ‚Üí DTT ‚Üí Type
ùìö R []       = ùüô
ùìö R (X ‚à∑ Xf) = K R X √ó ((x : X) ‚Üí ùìö R (Xf x))

ùìô :  Type ‚Üí DTT ‚Üí Type
ùìô R []       = ùüô
ùìô R (X ‚à∑ Xf) = J R X √ó ((x : X) ‚Üí ùìô R (Xf x))

\end{code}

 * œï ranges over the type K R X of quantifiers.
 * Œµ ranges over the type J R X of selection functions.

 * œït ranges over the type ùìö R Xt of quantifier trees.
 * Œµt ranges over the type ùìô R Xt of selection-function trees.

 * œïf ranges over the type (x : X) ‚Üí ùìö R (Xf x) of quantifier forests.
 * Œµf ranges over the type (x : X) ‚Üí ùìô R (Xf x) of selection-function forests.

Sequencing quantifiers and selections, as constructed in Definitions 2
and 12 of reference [1], but using our tree representation of games
instead:

\begin{code}

K-sequence : {Xt : DTT} {R : Type} ‚Üí ùìö R Xt ‚Üí K R (Path Xt)
K-sequence {[]}     ‚ü®‚ü©        q = q ‚ü®‚ü©
K-sequence {X ‚à∑ Xf} (œï :: œïf) q = œï (Œª x ‚Üí K-sequence {Xf x} (œïf x) (Œª xs ‚Üí q (x :: xs)))

J-sequence : {Xt : DTT} {R : Type} ‚Üí ùìô R Xt ‚Üí J R (Path Xt)
J-sequence {[]}     ‚ü®‚ü©        q = ‚ü®‚ü©
J-sequence {X ‚à∑ Xf} (Œµ :: Œµf) q = h :: t h
 where
  t : (x : X) ‚Üí Path (Xf x)
  t x = J-sequence {Xf x} (Œµf x) (Œª xs ‚Üí q (x :: xs))

  h : X
  h = Œµ (Œª x ‚Üí q (x :: t x))

\end{code}

The following is Definition 3 of the above reference [1].

A game is defined by a type R of outcomes, a game tree Xt, a
quantifier tree œït and an outcome function q:

\begin{code}

record Game : Type‚ÇÅ where
 constructor
  game

 field
  Xt  : DTT
  R   : Type
  œït  : ùìö R Xt
  q   : Path Xt ‚Üí R

open Game

\end{code}

We define the optimal outcome of a game to be the sequencing of its
quantifiers applied to the outcome function (Theorem 3.1 of [1]).

\begin{code}

optimal-outcome : (G : Game) ‚Üí G .R
optimal-outcome (game R Xt œït q) = K-sequence œït q

\end{code}

A strategy defines how to pick a path of a tree. The type Strategy of
all possible strategies is constructed as follows (Definition 4 of [1]):

\begin{code}

Strategy : DTT -> Type
Strategy []       = ùüô
Strategy (X ‚à∑ Xf) = X √ó ((x : X) ‚Üí Strategy (Xf x))

\end{code}

 * œÉ ranges over the type Strategy Xt of strategies for a
   dependent-type tree Xt.

 * œÉf ranges over the type (x : X) ‚Üí Strategy (Xf x) of strategy
   forests for a dependent-type forest Xf.

We get a path in the tree by following any given strategy:

\begin{code}

strategic-path : {Xt : DTT} ‚Üí Strategy Xt ‚Üí Path Xt
strategic-path {[]}    ‚ü®‚ü©         = ‚ü®‚ü©
strategic-path {X ‚à∑ Xf} (x :: œÉf) = x :: strategic-path {Xf x} (œÉf x)

\end{code}

In the notation of reference [1] above, Definition 5, a strategy œÉt
for a game (Xt,R,œït,q) is said to be optimal, or in subgame perfect
equillibrium (abbreviated sgpe), if for every partial play a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ
of length k, we have

   q(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,b‚Çñ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ),‚Ä¶,b‚Çô‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ))
 = œï‚Çñ(Œªx‚Çñ.q(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,x‚Çñ,b‚Çñ‚Çä‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,x‚Çñ),‚Ä¶,b‚Çô‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,x‚Çñ)))

where the sequence b is inductively defined by

  b‚±º(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ) = œÉ‚±º(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,b‚Çñ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ),‚Ä¶,b‚±º‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ))

for k ‚â§ j < n.

Intuitively, the strategy œÉ is optimal if the outcome

  q(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ,b‚Çñ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ),‚Ä¶,b‚Çô‚Çã‚ÇÅ(a‚ÇÄ,‚Ä¶,a‚Çñ‚Çã‚ÇÅ))

obtained by following œÉ is the best possible outcome as described by
the quantifier œï‚Çñ for each round k, considering all possible
deviations x‚Çñ from the strategy at that round.

For the purposes of our generalization of [1] to dependent games, it
is convenient to define this notion by induction on the game tree Xt:

\begin{code}

is-sgpe : {Xt : DTT} {R : Type} ‚Üí ùìö R Xt ‚Üí (Path Xt ‚Üí R) ‚Üí Strategy Xt ‚Üí Type
is-sgpe {[]}     ‚ü®‚ü©        q ‚ü®‚ü©         = ùüô
is-sgpe {X ‚à∑ Xf} (œï :: œïf) q (x‚ÇÄ :: œÉf) =

      (q (x‚ÇÄ :: strategic-path (œÉf x‚ÇÄ)) ‚â° œï (Œª x ‚Üí q (x :: strategic-path (œÉf x))))
    √ó
      ((x : X) ‚Üí is-sgpe {Xf x} (œïf x) (Œª (xs : Path (Xf x)) ‚Üí q (x :: xs)) (œÉf x))

\end{code}

In the above definition:

 * If the game tree is empty, then the strategy is empty, and we say
   that it is true that it is in sgpe, where "true" is represented by
   the unit type ùüô in propositions-as-types.

 * If the game tree has a root X followed by a forest Xf, then the
   strategy must be of the form x‚ÇÄ :: œÉf, where x‚ÇÄ is the first move
   according to the strategy, and where œÉf is a forest of strategies
   that depends on a deviation x.

   So the first part

     q (a :: strategic-path (œÉf a)) ‚â° œï (Œª x ‚Üí q (x :: strategic-path (œÉf x)))

   of the definition is as in the comment above, but with a partial
   play of length k=0, and the second (inductive) part, says that the
   substrategy œÉf x, for any deviation x, is in subgame perfect
   equillibrium in the subgame

     (Xf x , R , œïf x , Œª (xs : Path (Xf x)) ‚Üí q (x :: xs)).

As discussed above, we say that a strategy for a game is optimal if it
is in subgame perfect equilibrium.

\begin{code}

is-optimal : {G : Game} (œÉ : Strategy (G .Xt)) ‚Üí Type
is-optimal {game Xt R œït q} œÉ = is-sgpe {Xt} {R} œït q œÉ

\end{code}

The main lemma is that the optimal outcome is the same as the
application of the outcome function to the path induced by a strategy
in perfect subgame equilibrium.

The following is Theorem 3.1 of reference [1].

\begin{code}

sgpe-lemma : (R : Type) (Xt : DTT) (œït : ùìö R Xt) (q : Path Xt ‚Üí R) (œÉ : Strategy Xt)
           ‚Üí is-sgpe œït q œÉ
           ‚Üí K-sequence œït q ‚â° q (strategic-path œÉ)
sgpe-lemma R []       ‚ü®‚ü©        q ‚ü®‚ü©        ‚ü®‚ü©       = refl
sgpe-lemma R (X ‚à∑ Xf) (œï :: œït) q (a :: œÉf) (h :: t) = Œ≥
 where
  IH : (x : X) ‚Üí is-sgpe (œït x) (Œª xs ‚Üí q (x :: xs)) (œÉf x)
               ‚Üí K-sequence (œït x) (Œª xs ‚Üí q (x :: xs)) ‚â° q (x :: strategic-path (œÉf x))
  IH x = sgpe-lemma R (Xf x) (œït x) (Œª (xs : Path (Xf x)) ‚Üí q (x :: xs)) (œÉf x)

  Œ≥ = œï (Œª x ‚Üí K-sequence (œït x) (Œª xs ‚Üí q (x :: xs))) ‚â°‚ü® ap œï (dfunext fe (Œª x ‚Üí IH x (t x))) ‚ü©
      œï (Œª x ‚Üí q (x :: strategic-path (œÉf x)))         ‚â°‚ü® h ‚Åª¬π ‚ü©
      q (a :: strategic-path (œÉf a))                   ‚àé

\end{code}

This can be reformulated as follows in terms of the type of games:

\begin{code}

equillibrium-theorem : (G : Game) (œÉ : Strategy (G .Xt))
                     ‚Üí is-optimal œÉ
                     ‚Üí optimal-outcome G ‚â° G .q (strategic-path œÉ)
equillibrium-theorem (game R Xt œït q) = sgpe-lemma Xt R œït q

\end{code}

We now show how to use selection functions to compute a sgpe strategy.

We first convert a selection function into a quantifiers as in
Definition 10 of [1]:

\begin{code}

overline : {X R : Type} ‚Üí J R X ‚Üí K R X
overline Œµ = Œª p ‚Üí p (Œµ p)

\end{code}

The following is the application of overline to each selection
function of a tree:

\begin{code}

Overline : {Xt : DTT} {R : Type} ‚Üí ùìô R Xt ‚Üí ùìö R Xt
Overline {[]}    ‚ü®‚ü©         = ‚ü®‚ü©
Overline {X ‚à∑ Xf} (Œµ :: Œµs) = overline Œµ :: (Œª x ‚Üí Overline {Xf x} (Œµs x))

\end{code}

The following, which defines a strategy from given selection
functions, is defined in Theorem 5.4 of [1], with the difference that
here, for the moment, we consider only single-valued quantifiers.

\begin{code}

selection-strategy : {Xt : DTT} {R : Type} ‚Üí ùìô R Xt ‚Üí (Path Xt ‚Üí R) ‚Üí Strategy Xt
selection-strategy {[]}     ‚ü®‚ü©           q = ‚ü®‚ü©
selection-strategy {X ‚à∑ Xf} Œµt@(Œµ :: Œµf) q = x‚ÇÄ :: œÉf
 where
  x‚ÇÄ : X
  x‚ÇÄ = path-head (J-sequence Œµt q)

  œÉf : (x : X) ‚Üí Strategy (Xf x)
  œÉf x = selection-strategy {Xf x} (Œµf x) (Œª xs ‚Üí q (x :: xs))

\end{code}

The following definition is in Section 1 on [1].

\begin{code}

_is-a-selection-of_ : {X R : Type} ‚Üí J R X ‚Üí K R X ‚Üí Type
Œµ is-a-selection-of œï = overline Œµ ‚àº œï

\end{code}

We generalize it to selection-function and quantifier trees in the
obvious way, by induction:

\begin{code}

_are-selections-of_ : {Xt : DTT} {R : Type} ‚Üí ùìô R Xt ‚Üí ùìö R Xt ‚Üí Type
_are-selections-of_ {[]}     ‚ü®‚ü©        ‚ü®‚ü©        = ùüô
_are-selections-of_ {X ‚à∑ Xf} (Œµ :: Œµf) (œï :: œïf) = (Œµ is-a-selection-of œï)
                                                 √ó ((x : X) ‚Üí (Œµf x) are-selections-of (œïf x))

observation : {Xt : DTT} {R : Type} (Œµt : ùìô R Xt) (œït : ùìö R Xt)
            ‚Üí Œµt are-selections-of œït
            ‚Üí Overline Œµt ‚â° œït
observation {[]}     ‚ü®‚ü©        ‚ü®‚ü©        ‚ü®‚ü©        = refl
observation {X ‚à∑ Xf} (Œµ :: Œµf) (œï :: œïf) (a :: af) = Œ≥
 where
  IH : (x : X) ‚Üí Overline (Œµf x) ‚â° œïf x
  IH x = observation {Xf x} (Œµf x) (œïf x) (af x)

  I : overline Œµ ‚â° œï
  I = dfunext fe a

  II : (Œª x ‚Üí Overline (Œµf x)) ‚â° œïf
  II = dfunext fe IH

  Œ≥ : overline Œµ :: (Œª x ‚Üí Overline (Œµf x)) ‚â° œï :: œïf
  Œ≥ = ap‚ÇÇ _::_ I II

\end{code}

Notice that the converse is also true, that is, if Overline Œµt ‚â° œït
then Œµt are selection of œït, but we don't need this fact here.

\begin{code}

crucial-lemma : {Xt : DTT} {R : Type} (Œµt : ùìô R Xt) (q : Path Xt ‚Üí R)
              ‚Üí J-sequence Œµt q
              ‚â° strategic-path (selection-strategy Œµt q)
crucial-lemma {[]}     ‚ü®‚ü©           q = refl
crucial-lemma {X ‚à∑ Xf} Œµt@(Œµ :: Œµf) q = Œ≥
 where
  t : (x : X) ‚Üí Path (Xf x)
  t x = J-sequence {Xf x} (Œµf x) (Œª xs ‚Üí q (x :: xs))

  h : X
  h = Œµ (Œª x ‚Üí q (x :: t x))

  x‚ÇÄ : X
  x‚ÇÄ = path-head (J-sequence Œµt q)

  œÉf : (x : X) ‚Üí Strategy (Xf x)
  œÉf x = selection-strategy {Xf x} (Œµf x) (Œª xs ‚Üí q (x :: xs))

  IH : t h ‚â° strategic-path (œÉf x‚ÇÄ)
  IH = crucial-lemma (Œµf x‚ÇÄ) (Œª xs ‚Üí q (x‚ÇÄ :: xs))

  remark : h ‚â° x‚ÇÄ
  remark = refl

  Œ≥ : h :: t h ‚â° x‚ÇÄ :: strategic-path (œÉf x‚ÇÄ)
  Œ≥ = ap (h ::_) IH

selection-strategy-lemma : {Xt : DTT} {R : Type} (Œµt : ùìô R Xt) (q : Path Xt ‚Üí R)
                         ‚Üí is-sgpe (Overline Œµt) q (selection-strategy Œµt q)
selection-strategy-lemma {[]}     {R} ‚ü®‚ü©        q = ‚ü®‚ü©
selection-strategy-lemma {X ‚à∑ Xf} {R} (Œµ :: Œµf) q = h :: t
 where
  f g : X ‚Üí R
  f x = q (x :: J-sequence (Œµf x) (Œª xs ‚Üí q (x :: xs)))
  g x = q (x :: strategic-path (selection-strategy (Œµf x) (Œª xs ‚Üí q (x :: xs))))

  I : (x : X) ‚Üí J-sequence (Œµf x) (Œª xs ‚Üí q (x :: xs))
              ‚â° strategic-path (selection-strategy (Œµf x) (Œª xs ‚Üí q (x :: xs)))
  I x = crucial-lemma (Œµf x) (Œª xs ‚Üí q (x :: xs))

  II : f ‚â° g
  II = dfunext fe (Œª x ‚Üí ap (Œª - ‚Üí q (x :: -)) (I x))

  h : g (Œµ f) ‚â° g (Œµ g)
  h = ap (g ‚àò Œµ) II

  t : (x : X) ‚Üí is-sgpe (Overline (Œµf x)) (Œª xs ‚Üí q (x :: xs)) (selection-strategy (Œµf x) (Œª xs ‚Üí q (x :: xs)))
  t x = selection-strategy-lemma (Œµf x) (Œª xs ‚Üí q (x :: xs))


\end{code}

The following, which shows how to use selection functions to compute
optimal strategies, corresponds to Theorem 6.2 of [1].

\begin{code}

selection-strategy-theorem : {Xt : DTT} {R : Type} (Œµt : ùìô R Xt) (œït : ùìö R Xt) (q : Path Xt ‚Üí R)
                           ‚Üí Œµt are-selections-of œït
                           ‚Üí is-sgpe œït q (selection-strategy Œµt q)
selection-strategy-theorem Œµt œït q a = III
 where
  I : Overline Œµt ‚â° œït
  I = observation Œµt œït a

  II : is-sgpe (Overline Œµt) q (selection-strategy Œµt q)
  II = selection-strategy-lemma Œµt q

  III : is-sgpe œït q (selection-strategy Œµt q)
  III = transport (Œª - ‚Üí is-sgpe - q (selection-strategy Œµt q)) I II

\end{code}

Incomplete examples:

\begin{code}

no-repetitions : (n : ‚Ñï) (X : Type) ‚Üí DTT
no-repetitions zero X     = []
no-repetitions (succ n) X = X ‚à∑ Œª (x : X) ‚Üí no-repetitions n (Œ£ y Íûâ X , y ‚â¢ x )

open import Fin hiding ([] ; _‚à∑_ ; _::_ ; hd ; tl ; _++_)

Permutations : ‚Ñï ‚Üí Type
Permutations n = Path (no-repetitions n (Fin n))

example-permutation2 : Permutations 2
example-permutation2 = ùüé , (ùüè , Œª ()) , ‚ü®‚ü©

\end{code}

TODO. Define tic-tac-toe using no-repetitions and Fin 9.

TODO. Generalize the above to multi-valued quantifiers, using monads.
